{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "HEIGHT, WIDTH = 300,300\n",
    "N_CLASSES = 10\n",
    "_simple = 0\n",
    "\n",
    "def getImage(filename):\n",
    "    '''\n",
    "        instruct TF to read data \n",
    "    ''' \n",
    "    \n",
    "    # convert filenames to a queue for an input pipeline.\n",
    "    filenameQ = tf.train.string_input_producer([filename],num_epochs=None)\n",
    " \n",
    "    # object to read records\n",
    "    recordReader = tf.TFRecordReader()\n",
    "\n",
    "    # read the full set of features for a single example \n",
    "    key, fullExample = recordReader.read(filenameQ)\n",
    "\n",
    "    # parse the full example into its' component features.\n",
    "    features = tf.parse_single_example(\n",
    "        fullExample,\n",
    "        features={\n",
    "            'image/height': tf.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.FixedLenFeature([], tf.int64),\n",
    "            'image/colorspace': tf.FixedLenFeature([], dtype=tf.string,default_value=''),\n",
    "            'image/channels':  tf.FixedLenFeature([], tf.int64),            \n",
    "            'image/class/label': tf.FixedLenFeature([],tf.int64),\n",
    "            'image/class/text': tf.FixedLenFeature([], dtype=tf.string,default_value=''),\n",
    "            'image/format': tf.FixedLenFeature([], dtype=tf.string,default_value=''),\n",
    "            'image/filename': tf.FixedLenFeature([], dtype=tf.string,default_value=''),\n",
    "            'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value='')\n",
    "        })\n",
    "\n",
    "\n",
    "    # manipulate the label and image features\n",
    "\n",
    "    label = features['image/class/label']\n",
    "    image_buffer = features['image/encoded']\n",
    "\n",
    "    # Decode the jpeg\n",
    "    with tf.name_scope('decode_jpeg',[image_buffer], None):\n",
    "        # decode\n",
    "        image = tf.image.decode_jpeg(image_buffer, channels=3)\n",
    "        # and convert to single precision data type\n",
    "        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    # cast image into a single array, where each element corresponds to the greyscale\n",
    "    # value of a single pixel. \n",
    "    # the \"1-..\" part inverts the image, so that the background is black.\n",
    "\n",
    "    image=tf.reshape(1-tf.image.rgb_to_grayscale(image),[WIDTH*HEIGHT])\n",
    "\n",
    "    # re-define label as a \"one-hot\" vector \n",
    "    # it will be [0,1] or [1,0] here. \n",
    "    # This approach can easily be extended to more classes.\n",
    "    label=tf.stack(tf.one_hot(label-1, N_CLASSES))\n",
    "\n",
    "    return label, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Convolutional Neural Network Model\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d98ef2edd1eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mW_fc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWIDTH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mHEIGHT\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnFeatures2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnNeuronsfc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mb_fc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnNeuronsfc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-d98ef2edd1eb>\u001b[0m in \u001b[0;36mweight_variable\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# functions to init small positive weights and biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mweight_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0minitial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mtruncated_normal\u001b[0;34m(shape, mean, stddev, dtype, seed, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     rnd = gen_random_ops._truncated_normal(\n\u001b[0;32m--> 172\u001b[0;31m         shape_tensor, dtype, seed=seed1, seed2=seed2)\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0mmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstddev_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_random_ops.py\u001b[0m in \u001b[0;36m_truncated_normal\u001b[0;34m(shape, dtype, seed, seed2, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \"\"\"\n\u001b[1;32m    335\u001b[0m   result = _op_def_lib.apply_op(\"TruncatedNormal\", shape=shape, dtype=dtype,\n\u001b[0;32m--> 336\u001b[0;31m                                 seed=seed, seed2=seed2, name=name)\n\u001b[0m\u001b[1;32m    337\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    588\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    589\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 61\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"
     ]
    }
   ],
   "source": [
    "# associate the \"label\" and \"image\" objects with the corresponding features read from \n",
    "# a single example in the training data file\n",
    "label, image = getImage(\"data/synthetic/train-00000-of-00001\")\n",
    "\n",
    "# and similarly for the validation data\n",
    "vlabel, vimage = getImage(\"data/synthetic/validation-00000-of-00001\")\n",
    "\n",
    "# associate the \"label_batch\" and \"image_batch\" objects with a randomly selected batch---\n",
    "# of labels and images respectively\n",
    "imageBatch, labelBatch = tf.train.shuffle_batch(\n",
    "    [image, label], batch_size=100,\n",
    "    capacity=2000,\n",
    "    min_after_dequeue=1000)\n",
    "\n",
    "# and similarly for the validation data \n",
    "vimageBatch, vlabelBatch = tf.train.shuffle_batch(\n",
    "    [vimage, vlabel], batch_size=100,\n",
    "    capacity=2000,\n",
    "    min_after_dequeue=1000)\n",
    "\n",
    "# interactive session allows inteleaving of building and running steps\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# x is the input array, which will contain the data from an image \n",
    "# this creates a placeholder for x, to be populated later\n",
    "x = tf.placeholder(tf.float32, [None, WIDTH*HEIGHT])\n",
    "# similarly, we have a placeholder for true outputs (obtained from labels)\n",
    "y_ = tf.placeholder(tf.float32, [None, N_CLASSES])\n",
    "\n",
    "if _simple:\n",
    "    # run simple model y=Wx+b given in TensorFlow \"MNIST\" tutorial\n",
    "    print (\"Running simple model: y=Wx+b\")\n",
    "\n",
    "    # initialise weights and biases to zero\n",
    "    # W maps input to output so is of size: (number of pixels) * (Number of Classes)\n",
    "    W = tf.Variable(tf.zeros([WIDTH*HEIGHT, N_CLASSES]))\n",
    "    # b is vector which has a size corresponding to number of classes\n",
    "    b = tf.Variable(tf.zeros([N_CLASSES]))\n",
    "\n",
    "    # define output calc (for each class) y = softmax(Wx+b)\n",
    "    # softmax gives probability distribution across all classes\n",
    "    y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "else:\n",
    "    # run convolutional neural network model given in \"Expert MNIST\" TensorFlow tutorial\n",
    "\n",
    "    # functions to init small positive weights and biases\n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    # set up \"vanilla\" versions of convolution and pooling\n",
    "    def conv2d(x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    print (\"Running Convolutional Neural Network Model\")\n",
    "    nFeatures1=32\n",
    "    nFeatures2=64\n",
    "    nNeuronsfc=1024\n",
    "\n",
    "    # use functions to init weights and biases\n",
    "    # nFeatures1 features for each patch of size 5x5\n",
    "    # SAME weights used for all patches\n",
    "    # 1 input channel\n",
    "    W_conv1 = weight_variable([5, 5, 1, nFeatures1])\n",
    "    b_conv1 = bias_variable([nFeatures1])\n",
    "\n",
    "    # reshape raw image data to 4D tensor. 2nd and 3rd indexes are W,H, fourth \n",
    "    # means 1 colour channel per pixel\n",
    "    # x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    x_image = tf.reshape(x, [-1,WIDTH,HEIGHT,1])\n",
    "\n",
    "\n",
    "    # hidden layer 1 \n",
    "    # pool(convolution(Wx)+b)\n",
    "    # pool reduces each dim by factor of 2.\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    # similarly for second layer, with nFeatures2 features per 5x5 patch\n",
    "    # input is nFeatures1 (number of features output from previous layer)\n",
    "    W_conv2 = weight_variable([5, 5, nFeatures1, nFeatures2])\n",
    "    b_conv2 = bias_variable([nFeatures2])\n",
    "\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "    # denseley connected layer. Similar to above, but operating\n",
    "    # on entire image (rather than patch) which has been reduced by a factor of 4 \n",
    "    # in each dimension\n",
    "    # so use large number of neurons \n",
    "\n",
    "    # check our dimensions are a multiple of 4\n",
    "    if (WIDTH%4 or HEIGHT%4):\n",
    "        print (\"Error: width and height must be a multiple of 4\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    W_fc1 = weight_variable([(WIDTH/4) * (HEIGHT/4) * nFeatures2, nNeuronsfc])\n",
    "    b_fc1 = bias_variable([nNeuronsfc])\n",
    "\n",
    "    # flatten output from previous layer\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, (WIDTH/4) * (HEIGHT/4) * nFeatures2])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # reduce overfitting by applying dropout\n",
    "    # each neuron is kept with probability keep_prob\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # create readout layer which outputs to nClass categories\n",
    "    W_fc2 = weight_variable([nNeuronsfc, nClass])\n",
    "    b_fc2 = bias_variable([nClass])\n",
    "\n",
    "    # define output calc (for each class) y = softmax(Wx+b)\n",
    "    # softmax gives probability distribution across all classes\n",
    "    # this is not run until later\n",
    "    y=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    \n",
    "# measure of error of our model\n",
    "# this needs to be minimised by adjusting W and b\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# define training step which minimises cross entropy\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# argmax gives index of highest entry in vector (1st axis of 1D tensor)\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "# get mean of all entries in correct prediction, the higher the better\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, training accuracy 0.25\n",
      "step 200, training accuracy 0.16\n",
      "step 300, training accuracy 0.12\n",
      "step 400, training accuracy 0.14\n",
      "step 500, training accuracy 0.21\n",
      "step 600, training accuracy 0.15\n",
      "step 700, training accuracy 0.28\n",
      "step 800, training accuracy 0.19\n",
      "step 900, training accuracy 0.17\n",
      "step 1000, training accuracy 0.15\n"
     ]
    }
   ],
   "source": [
    "# initialize the variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# start the threads used for reading files\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "\n",
    "# start training\n",
    "nSteps=1000\n",
    "for i in range(nSteps):\n",
    "\n",
    "    batch_xs, batch_ys = sess.run([imageBatch, labelBatch])\n",
    "\n",
    "    # run the training step with feed of images\n",
    "    if _simple:\n",
    "      train_step.run(feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    else:\n",
    "      train_step.run(feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "\n",
    "\n",
    "    if (i+1)%100 == 0: # then perform validation \n",
    "\n",
    "      # get a validation batch\n",
    "      vbatch_xs, vbatch_ys = sess.run([vimageBatch, vlabelBatch])\n",
    "      if _simple:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "          x:vbatch_xs, y_: vbatch_ys})\n",
    "      else:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "          x:vbatch_xs, y_: vbatch_ys, keep_prob: 1.0})\n",
    "      print(\"step %d, training accuracy %g\"%(i+1, train_accuracy))\n",
    "\n",
    "\n",
    "# finalise \n",
    "coord.request_stop()\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
